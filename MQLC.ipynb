{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from d2l import torch as d2l\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report,hamming_loss\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"\")\n",
    "data = sklearn.utils.shuffle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "bert = BertModel.from_pretrained(\"bert-base-chinese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mydataset(Dataset):\n",
    "    def __init__(self, content, label,tokenizer, label_vocab):\n",
    "        self.content = content\n",
    "        self.label = label\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label_vocab = label_vocab\n",
    "    def __getitem__(self, index):\n",
    "        # 先把每个list的每个str转变成label\n",
    "        con = self.content[index]\n",
    "        lab = self.label[index]\n",
    "        # 把一个label作为一个index\n",
    "        lab_split = lab.split('/')\n",
    "        lab_index = [self.label_vocab[i] for i in lab_split]\n",
    "        return con, lab_index\n",
    "        # 用自己定义的方式处理数据，按照每一个batch来填充，并且在中文部分把开始和终止的符号也加上\n",
    "        # content用bert，label用自己的\n",
    "    def batch_data_process(self,batch_datas):\n",
    "        content_vocab = {\"<PAD>\":0}\n",
    "        con_index, lab_index = [], []\n",
    "        con_len, lab_len = [], []\n",
    "        for con,_ in batch_datas:\n",
    "            con_len.append(len(con))\n",
    "        max_con_len = max(con_len)\n",
    "        \n",
    "        for con, lab in batch_datas:\n",
    "            con_index.append(self.tokenizer.encode(con,add_special_tokens=True,truncation=True,\n",
    "                                                    padding='max_length',max_length=max_con_len+2))   #maxlen要换\n",
    "            lab_index.append(lab)\n",
    "            #con_len.append(len(con))\n",
    "            lab_len.append(len(lab))\n",
    "        #max_con_len = max(con_len)\n",
    "        max_lab_len = max(lab_len)\n",
    "        # 注意啊，这边我用的是最大长度作为num_steps，李沐的是设置一个num_steps,其实本质是一样的，就是小于numsteps的填充，大于的截断\n",
    "        \n",
    "        lab_index = [i+[self.label_vocab[\"<EOS>\"]]+[self.label_vocab[\"<PAD>\"]]*(max_lab_len-len(i)) for i in lab_index]\n",
    "        con_index = torch.tensor(con_index)\n",
    "        lab_index = torch.tensor(lab_index)\n",
    "        con_valid_len = (con_index != content_vocab[\"<PAD>\"]).type(torch.int32).sum(1)\n",
    "        lab_valid_len = (lab_index != label_vocab[\"<PAD>\"]).type(torch.int32).sum(1)\n",
    "        return con_index, con_valid_len, lab_index, lab_valid_len\n",
    "    def __len__(self):\n",
    "        return len(self.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b = list(data[\"content\"]),list(data[\"label\"])\n",
    "label_vocab = {\n",
    "    \"内容准确\":0,\"内容不准确\":1,\"内容完整\":2,\"内容不完整\":3, \"难度适中\":4,\"难度不合理\":5, \"逻辑合理\":6,\"逻辑不合理\":7,\n",
    "    \"更新及时\":8,\"更新不及时\":9,\"内容有用\":10, \"内容没用\":11, \"内容新颖\":12,\"内容普通\":13, \"<BOS>\":14, \"<PAD>\":15,\"<EOS>\":16\n",
    "}\n",
    "dataset=mydataset(a,b,tokenizer,label_vocab)\n",
    "dataloader=DataLoader(dataset, batch_size=4, shuffle=False, collate_fn=dataset.batch_data_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [ \"内容准确\",\"内容不准确\",\"内容完整\",\"内容不完整\", \"难度适中\",\"难度不合理\", \"逻辑合理\",\"逻辑不合理\",\n",
    "    \"更新及时\",\"更新不及时\",\"内容有用\", \"内容没用\", \"内容新颖\",\"内容普通\", \"<BOS>\", \"<PAD>\",\"<EOS>\"]\n",
    "label = torch.tensor([[ 3,  7, 16],\n",
    "        [10, 16, 15],\n",
    "        [10, 16, 15],\n",
    "        [ 1, 16, 15]])\n",
    "batch_labels_embedding = torch.zeros(1,label.size(1),768)\n",
    "for oneBatch in label:\n",
    "    o = torch.zeros(1,768)\n",
    "    for words in oneBatch:\n",
    "        i = int(words.item()) \n",
    "        print(i)\n",
    "        l = labels[i]      \n",
    "        print(l)\n",
    "        token = tokenizer.encode(l, return_tensors=\"pt\",padding=True)  \n",
    "        label_embedding = bert(token).pooler_output           \n",
    "        print(\"labels_embedding\",label_embedding.size())\n",
    "        o = torch.cat((o,label_embedding),0)\n",
    "        print(\"o \",o.size())\n",
    "    labels_embedding = o[1:,:].unsqueeze(dim=0)    \n",
    "    print(\"labels_embedding\",labels_embedding.size())\n",
    "    batch_labels_embedding = torch.cat((batch_labels_embedding,labels_embedding),0)\n",
    "    print(batch_labels_embedding.size())\n",
    "batch_labels_embedding_final = batch_labels_embedding[1:,:,:]\n",
    "print(batch_labels_embedding_final.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelll = torch.tensor([[[3,0],[7,0],[1,6]],\n",
    "        [[1,0],[1,6],[1,5]]])\n",
    "print(labelll.size())\n",
    "labellll = labelll[1:,:,:]\n",
    "print(labellll,labellll.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myBert(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-chinese\")\n",
    "\n",
    "    def forward(self,batch_x):\n",
    "        bert_out = self.bert(batch_x)\n",
    "        return bert_out\n",
    "class Seq2SeqEncoder(nn.Module):\n",
    "    def __init__(self, mybert, num_hiddens, num_layers, dropout=0, **kwargs):\n",
    "        super(Seq2SeqEncoder, self).__init__(**kwargs)\n",
    "        self.bert = mybert\n",
    "        self.rnn = nn.LSTM(embed_size, num_hiddens, num_layers, dropout=dropout)\n",
    "\n",
    "    def forward(self, input, *args):\n",
    "        embedd_input = self.bert(input)[0]\n",
    "        embedd_input = embedd_input.permute(1, 0, 2)\n",
    "        output, state = self.rnn(embedd_input)\n",
    "        return output, state\n",
    "class AttentionDecoder(d2l.Decoder):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionDecoder, self).__init__(**kwargs)\n",
    "    @property\n",
    "    def attention_weights(self):\n",
    "        raise NotImplementedError\n",
    "class Seq2SeqAttentionDecoder(AttentionDecoder):\n",
    "    def __init__(self,tokenizer, mybert, vocab_size, embed_size, num_hiddens, num_layers,\n",
    "                 dropout=0, **kwargs):\n",
    "        super(Seq2SeqAttentionDecoder, self).__init__(**kwargs)\n",
    "        self.attention = d2l.AdditiveAttention(\n",
    "            num_hiddens, num_hiddens, num_hiddens, dropout)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.bert = mybert\n",
    "        self.rnn = nn.LSTM(\n",
    "            embed_size + num_hiddens, num_hiddens, num_layers,\n",
    "            dropout=dropout)\n",
    "        self.dense = nn.Linear(num_hiddens, vocab_size)\n",
    "\n",
    "    def init_state(self, enc_outputs, enc_valid_lens, *args):\n",
    "        outputs, hidden_state = enc_outputs\n",
    "        return (outputs.permute(1, 0, 2), hidden_state, enc_valid_lens)\n",
    "    \n",
    "    def label_represtation_embedding(self,X):\n",
    "        labels = [ \"内容准确\",\"内容不准确\",\"内容完整\",\"内容不完整\", \"难度适中\",\"难度不合理\", \"逻辑合理\",\"逻辑不合理\",\"更新及时\",\"更新不及时\",\"内容有用\", \"内容没用\", \"内容新颖\",\"内容普通\", \"<BOS>\", \"<PAD>\",\"<EOS>\"]\n",
    "        batch_labels_embedding = torch.zeros(1,X.size(1),768).cuda()\n",
    "        for oneBatch in X:\n",
    "            o = torch.zeros(1,768).cuda()\n",
    "            for words in oneBatch:\n",
    "                i = int(words.item())  \n",
    "                l = labels[i]      \n",
    "                token = self.tokenizer.encode(l, return_tensors=\"pt\",padding=True).cuda()  \n",
    "                label_embedding = self.bert(token)[1].cuda()           \n",
    "                o = torch.cat((o,label_embedding),0)\n",
    "            labels_embedding = o[1:,:].unsqueeze(dim=0).cuda()     \n",
    "            batch_labels_embedding = torch.cat((batch_labels_embedding,labels_embedding),0)\n",
    "        batch_labels_embedding_final = batch_labels_embedding[1:,:,:].cuda()\n",
    "        return batch_labels_embedding_final\n",
    "    \n",
    "    def forward(self, X, state):\n",
    "        enc_outputs, hidden_state, enc_valid_lens = state\n",
    "        \n",
    "        X = self.label_represtation_embedding(X).permute(1, 0, 2)\n",
    "        outputs, self._attention_weights = [], []\n",
    "        for x in X:\n",
    "            query = torch.unsqueeze(hidden_state[-1], dim=1)\n",
    "            context = self.attention(\n",
    "                query, enc_outputs, enc_outputs, enc_valid_lens)\n",
    "            x = torch.cat((context, torch.unsqueeze(x, dim=1)), dim=-1)\n",
    "            out, hidden_state = self.rnn(x.permute(1, 0, 2), hidden_state)\n",
    "            outputs.append(out)\n",
    "            self._attention_weights.append(self.attention.attention_weights)\n",
    "        outputs = self.dense(torch.cat(outputs, dim=0))\n",
    "        return outputs.permute(1, 0, 2), [enc_outputs, hidden_state,\n",
    "                                          enc_valid_lens]\n",
    "    \n",
    "    @property\n",
    "    def attention_weights(self):\n",
    "        return self._attention_weights\n",
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(EncoderDecoder, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    def forward(self ,enc_X, dec_X, *args):\n",
    "        enc_outputs = self.encoder(enc_X, *args)\n",
    "        dec_state = self.decoder.init_state(enc_outputs, *args)\n",
    "        return self.decoder(dec_X, dec_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_mask(X, valid_len, value=0):\n",
    "    maxlen = X.size(1)\n",
    "    mask = torch.arange((maxlen), dtype=torch.float32,\n",
    "                        device=X.device)[None, :] < valid_len[:, None]\n",
    "    X[~mask] = value\n",
    "    return X\n",
    "class MaskedSoftmaxCELoss(nn.CrossEntropyLoss):\n",
    "    def forward(self, pred, label, valid_len):\n",
    "        weights = torch.ones_like(label)\n",
    "        weights = sequence_mask(weights, valid_len)\n",
    "        self.reduction = 'none'\n",
    "        unweighted_loss = super(MaskedSoftmaxCELoss, self).forward(\n",
    "            pred.permute(0, 2, 1), label)          \n",
    "        weighted_loss = (unweighted_loss * weights).mean(dim=1)\n",
    "        return weighted_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_valid(net, train_data_iter,valid_data_iter, lr, num_epochs, tgt_vocab, device):\n",
    "    def xavier_init_weights(m):\n",
    "        if type(m) == nn.Linear:\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "        if type(m) == nn.GRU:\n",
    "            for param in m._flat_weights_names:\n",
    "                if \"weight\" in param:\n",
    "                    nn.init.xavier_uniform_(m._parameters[param])\n",
    "    \n",
    "\n",
    "    net.apply(xavier_init_weights)\n",
    "    net.to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    loss = MaskedSoftmaxCELoss()\n",
    "    train_loss = 0 \n",
    "    valid_loss = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        net.train()\n",
    "        for batch in train_data_iter:\n",
    "            X, X_valid_len, Y, Y_valid_len = [x.to(device) for x in batch]\n",
    "            Y_hat, _ = net(X, Y, X_valid_len)\n",
    "            l = loss(Y_hat, Y, Y_valid_len)\n",
    "            optimizer.zero_grad()\n",
    "            l.sum().backward()\n",
    "            l = l.sum()\n",
    "            nn.utils.clip_grad_norm_(net.parameters(),1)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            train_loss += l.item() \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            net.eval()\n",
    "            for batch in valid_data_iter:\n",
    "                X, X_valid_len, Y, Y_valid_len = [x.to(device) for x in batch]\n",
    "                bos = torch.tensor([tgt_vocab['<BOS>']] * Y.shape[0],\n",
    "                              device=device).reshape(-1, 1)\n",
    "                dec_input = torch.cat([bos, Y[:, :-1]], 1)\n",
    "                Y_hat, _ = net(X, dec_input, X_valid_len)\n",
    "                l = loss(Y_hat, Y, Y_valid_len)\n",
    "                l = l.sum()\n",
    "                valid_loss += l.item()\n",
    "        train_loss = train_loss/len(train_data_iter)\n",
    "        valid_loss = valid_loss/len(train_data_iter)\n",
    "#         if (epoch + 1) % 5 == 0:\n",
    "        print('Epoch: {} \\t Avgerage Training Loss: {:.6f} \\tAverage Validation Loss: {:.6f}'\n",
    "                      .format(epoch, train_loss, valid_loss))\n",
    "\n",
    "\n",
    "        # save checkpoint as best model\n",
    "    torch.save(net.state_dict(), \"./data/net_parameter.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_vocab = {\n",
    "    \"内容准确\":0,\"内容不准确\":1,\"内容完整\":2,\"内容不完整\":3, \"难度适中\":4,\"难度不合理\":5, \"逻辑合理\":6,\"逻辑不合理\":7,\n",
    "    \"更新及时\":8,\"更新不及时\":9,\"内容有用\":10, \"内容没用\":11, \"内容新颖\":12,\"内容普通\":13, \"<BOS>\":14, \"<PAD>\":15,\"<EOS>\":16\n",
    "}\n",
    "embed_size, num_hiddens, num_layers, dropout = \n",
    "batch_size, num_steps = \n",
    "lr, num_epochs =\n",
    "device = d2l.try_gpu()\n",
    "mybert = myBert()\n",
    "encoder = Seq2SeqEncoder(mybert,num_hiddens, num_layers,\n",
    "                        dropout)\n",
    "decoder = Seq2SeqAttentionDecoder(tokenizer,mybert,len(label_vocab), embed_size, num_hiddens, num_layers,\n",
    "                        dropout)\n",
    "net = EncoderDecoder(encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=mydataset(train_content,train_label,tokenizer,label_vocab)\n",
    "valid_dataset=mydataset(valid_content,valid_label,tokenizer,label_vocab)\n",
    "train_dataloader=DataLoader(train_dataset, batch_size=batch_size,shuffle=True ,collate_fn=train_dataset.batch_data_process)\n",
    "valid_dataloader=DataLoader(valid_dataset, batch_size=batch_size,shuffle=True ,collate_fn=valid_dataset.batch_data_process)\n",
    "checkpoint_path = ''\n",
    "best_model = ''\n",
    "train_and_valid(net, train_dataloader,valid_dataloader, lr, num_epochs, label_vocab, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_seq2seq(net, src_sentence, src_vocab, tgt_vocab,\n",
    "                    device, save_attention_weights=True):\n",
    "    net.eval()\n",
    "    src_tokens = tokenizer.encode(src_sentence,add_special_tokens=True,truncation=True,padding='max_length',max_length=55)\n",
    "    len_src_tokens = 0\n",
    "    for i in src_tokens:\n",
    "        if i != 0:\n",
    "            len_src_tokens+=1\n",
    "    enc_valid_len = torch.tensor([len_src_tokens], device=device)\n",
    "    enc_X = torch.unsqueeze(\n",
    "        torch.tensor(src_tokens, dtype=torch.long, device=device), dim=0)\n",
    "    enc_outputs = net.encoder(enc_X, enc_valid_len)\n",
    "    dec_state = net.decoder.init_state(enc_outputs, enc_valid_len)\n",
    "    dec_X = torch.unsqueeze(torch.tensor(\n",
    "        [tgt_vocab['<BOS>']], dtype=torch.long, device=device), dim=0)\n",
    "    output_seq, attention_weight_seq = [], []\n",
    "    output_tokens = []\n",
    "    while True:\n",
    "        Y, dec_state = net.decoder(dec_X, dec_state)\n",
    "        dec_X = Y.argmax(dim=2)\n",
    "        pred = dec_X.squeeze(dim=0).type(torch.int32).item()\n",
    "        if save_attention_weights:\n",
    "            attention_weight_seq.append(net.decoder.attention_weights)\n",
    "        if pred == tgt_vocab['<EOS>']:\n",
    "            break\n",
    "        output_seq.append(pred)\n",
    "    return output_seq, attention_weight_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_copy = copy.deepcopy(net)\n",
    "model.load_state_dict(torch.load(''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_predict,label_predict = list(data[\"content\"]),list(data[\"label\"])\n",
    "label_pre = []\n",
    "for c, l in zip(content_predict, label_predict):\n",
    "    labels, attention_weight_seq = predict_seq2seq(\n",
    "        net, c, tokenizer, label_vocab, device)\n",
    "    label_pre.append(labels)\n",
    "label_pre_pad = []\n",
    "for L in label_pre:\n",
    "    l = np.zeros(14,dtype=np.int).tolist()\n",
    "    for j in L:\n",
    "        l[j] = 1\n",
    "    label_pre_pad.append(l)\n",
    "label_data = list(data[\"label\"])[1750:]\n",
    "label_data_pad = []\n",
    "for L in label_data:\n",
    "    lab_split = L.split('/')\n",
    "    lab_index = [label_vocab[i] for i in lab_split]\n",
    "    l = np.zeros(14,dtype=np.int).tolist()\n",
    "    for j in lab_index:\n",
    "        l[j] = 1\n",
    "    label_data_pad.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(label_data_pad, label_pre_pad)\n",
    "f1_score_micro = f1_score(label_data_pad, label_pre_pad, average='micro')\n",
    "f1_score_macro = f1_score(label_data_pad, label_pre_pad, average='macro')\n",
    "ham_loss = hamming_loss(label_data_pad,label_pre_pad)\n",
    "print(f\"Accuracy Score = {accuracy}\")\n",
    "print(f\"Hamming losss = {ham_loss}\")\n",
    "print(f\"F1 Score (Micro) = {f1_score_micro}\")\n",
    "print(f\"F1 Score (Macro) = {f1_score_macro}\")\n",
    "print(classification_report(label_data_pad, label_pre_pad))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
